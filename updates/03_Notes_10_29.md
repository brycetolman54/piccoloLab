---
geometry: "margin=1in"
---

# Contents
<!--{-->

1. [Project Overview](#part-1)
2. [MS Thesis](#part-2)
3. [Adversarial Autoencoders to Integrate cell lines](#part-3)
4. [Adversarial deconfounding autoencoder for embeddings](#part-4)
5. [Deep learning embedding improves batch effect estimation](#part-5)
6. [Removing Batch effects using GANs](#part-6)
7. [General Notes](#part-7)
8. [Thoughts about the algorithm](#part-8)
9. [Further Resources](#part-9)

<!--}-->


## Project Overview {#part-1}
<!--{-->

- The idea behind this project is to find a way to automatically combine different datasets for use in large studies. 
    - The goal is to remove the noise that arises from the different platforms (or from different batches, or from whatever source we choose) while maintaining the biological data that is in the data.
    - We are not so interested in making a model that works well on all the data sets as we are in finding a way to combine the data sets.
- In order to do this, we are trying to find some sort of deep learning algorithm that we can use to combine the datasets, rather than using statistical methods like ComBat or others that have been used to combine the datasets.
- We are going to combine ideas behind deep learning and other machine learning models about learning from data sets, but in the end we are going to write our own algorithms to do what we want.
- One type of model that might be nice to use is an adversarial model.
    - This type of model tries to optimize two different functions at once. So, we could do something that tries to maximize the accuracy of predictions on our data set while also minimizing the signal from the noise.
- There are many different sources of noise that can arise between different batches of data. These can be large (like different platforms being used to collect the data), or they can be small (like data being collected at different hospitals). We can focus on either removing all sources of noise, or getting rid of only the large ones. 
- We are thinking of doing supervised or unsupervised learning.
    - This means we can either tell the algorithm where we think the sources of noise are that we want it to reduce, or we can just feed it the data sets and see how it handles the sets and combines, to see where it chooses to reduce noise.
    - The supervised version is supervised in that we are giving it places to find and reduce noise.
    - Another way that the algorithm would be considered supervised (if we are doing an adversarial network) is because we are providing real data for the Discriminator to distinguish from the generated data. 

- I guess one question I have is this:
    - Are we trying to just combine the data sets for learning from only them?
    - Or are we trying to make it so we can combine our data sets into a training set for use with future predictions on novel data?
    - Is that kind of the same thing? I don't know how much that will affect how we want to set up the algorithm.

- We are trying to make a framework/algorithm for combining data sets, yes?
    - If we are testing things out with the Breast Cancer data sets, we can't rely on the models we make for other data sets, because they have different columns.
    - I guess we could make some sort of an algorithm to use everywhere, and just have to set up the network/model to deal with the right number of features, given the data set we are looking at.
    - Then we have to worry about making people prepare their data sets properly before using our algorithm.
        - Could we somehow automate this preprocessing? I don't think that is something to worry about right now.

<!--}-->


## Thesis {#part-2}
<!--{-->

- The thesis can be found [here](https://scholarsarchive.byu.edu/etd/7521/)

- Our study is looking to overcome the problem with integrating data sets, whether we know the covariates (supervised) or not (unsupervised) that are causing the difference between the data sets. 
- For the breast cancer data, we are using data from two different platforms. 
    - That difference between platforms is a covariate that we know and can select, but there may be other sources of noise that we can't know straight out.
- When we get to looking at combining datasets from other studies, the known covariate won't necessarily be a difference in platform.
    - So, our model should focus on finding the noise itself rather than being told where it is.
        - One way we can test this in our data is to try combining only data sets that come from the same platform (thus having it find small noise and differences that are not so obvious), then having it combine things from different platforms (thus having it find larger noise and differences, even without us telling it what to look for).
- There can exist nonlinear relationships in the data that normal statistical methods cannot find/solve. 
    - Thus, we want to use a neural network to find these relationships and remove them (since neural networks are able to do so).
- They used here a classifier to determine if their integration had worked well enough. 
    - They classified based on batch: the lower the batch classification accuracy, the better the integration was. This seems like a useful thing to do.

<!--}-->


## Adversarial Autoencoders to Integrate cell lines {#part-3}
<!--{-->

- The paper can be found [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC10056671/)

- They rely on their data having similar cells across the batches, this is what they use to correct the batch effect. We do not have something similar in our data, so I don't think their method would be too useful to us.
    - But, maybe something in the set up of their model would be?
- I think it could be useful to select a certain data set as the "standard". like the anchor batch they are using in their study. We try to make all other data sets conform to that "standard".
    - Ideally, the standard, like their anchor, would have good variation in it.
    - Also, it would be nice if it had a lot of data points (something like METABRIC for the breast cancer data)
- Man, after looking through this paper, it seems very similar to the ideas I have had, though it is slightly different.
    - If Dr. Piccolo decides that my idea is one to run with, I will need to come back to this and make sure my model isn't just a copy of theirs.
        - Again, I don't think it is, but it seems similar.

<!--}-->


## Adversarial deconfounding autoencoder for embeddings {#part-4}
<!--{-->

- The paper can be found [here](https://academic.oup.com/bioinformatics/article/36/Supplement_2/i573/6055930?rss=1)

- They talk about confounders that contaminate our data and make it hard to extract biologically meaningful data.
    - They talk about batch effects and noise, but also out-of-interest biological variables (like age and sex).
    - My thought is that we would take care of these sources of variation without having to explicitly address them
- They talk about how the latent nodes can get contaminated by the confounders.
    - I feel like this would be an issue with my idea.
    - If the latent space representation distribution of the standard is contaminated by confounders, will that not leave me nothing good to work with?
- Their network seems to need to know the confounder. I feel like my idea would work without knowing the confounders. That is a point for mine if I am right.
    - In my model, the Discriminator is trying to tell which batch something comes from, not a specific confounder.
        - In doing that, I hope to let the model find all confounders that are in the batch an remove them as a whole by removing the difference between batches.
- Their model is different in that they are trying to minimize the ability to predict the specific confounder from the latent space.
    - My model is more along the lines of using that latent space to make the sets conform.
    - Then, it decodes that latent space and sees if it can discriminate between whole batches.
- A question they deal with that I haven't considered:
    - What do we prioritize in our combination: saving the biological data or removing the difference between our data sets?
- They are trying to reduce the data to a form that doesn't include the confounders.
    - My thoughts of a model lead to something that fully reconstructs the data, but makes data sets conform.
    - Do we want reduced data?
        - That would mean we stop at the latent space and don't worry about decoding the information
            - Honestly, that is mostly what my model does. The decoding is no serious part, it is just used to give us back the data in full, but we could do without it...
            - I like the idea of decoding better, though, I'll see what Dr. Piccolo thinks.
- Thinking back to ComBat:
    - I feel like my method is something similar, just using neural networks and moving around the latent space representations
- One point they say is a strong point of their network that others don't have:
    - Their model can work with any confounder that has nominal or continuous data.
    - I don't know if my idea could do this same thing. I think it could, but I am unsure.
- They talk about a strength of their model being that they are fitting the adversary on the latent space (the embedding space).
    - This was my idea as well, though I think we go about it in different ways.
- Their model is able to take novel data that has not passed through an encoder ever and get a similar distribution to the training data.
    - My model would not do that. My thought was to train a new encoder for each novel data set
        - What if we didn't though? What if we did just have data sets representative from each platform in the training set?
            - Then we wouldn't have to retrain an encoder for each new set
            - But then we would have to specify what the confounder is, which we may not know.
            - Plus, if the goal is to just to combine data sets, we don't necessarily need to remove those confounders...
- Even still, their accuracy was weak (on the novel data). Could we do better?
    - But, their model does better than others in generalizing to new domains still. So that is good.
- They say a limitation is that their model does not account for multiple confounders at once. 
    - I think that my idea would be able to do so.

<!--}-->


## Deep learning embedding improving batch effect estimation {#part-5}
<!--{-->

- The paper can be found [here](https://academic.oup.com/bioinformatics/article/39/8/btad479/7240486?login=false)

- Their network, again, seems similar to what I had in mind.
    - They take their data, preprocess it, run it through an Encoder/Decoder to get the latent space representation (The embedding), then put that embedding into two other networks
        - One is to see if they can find the batch
        - One is to see if they can still find the biological data of interest
        - The difference is this: I am making an embedding from one data set and then making all future data sets conform to that embedding
            - Is that sufficiently different? I don't know.
- I feel like they are feeding their whole data set into the Encoder/Decoder at once. 
    - The method I have proposed takes only one data set at a time to create an embedding. Would it be more robust to do it all at once?
        - I don't know. I feel like it would take less time, but then you are giving less consideration to each data set you are trying to combine.
- They use PCA to show how they are correcting for batch effects (so they can plot in fewer dimensions). I think that is something to do with our model too.

<!--}-->


## Removing batch effects using GANs {#part-6}
<!--{-->

- The paper can be found [here](https://arxiv.org/pdf/1901.06654)

- 

<!--}-->


## General Notes {#part-7}
<!--{-->

- The idea behind a <b>Generative Adversarial Network (GAN)</b>:
    - We give the GAN real data and fake data (the fake data is constructed from a different network)
    - The Discriminator tries to tell these two apart (it is working to minimize error in predicting the class [real or fake] for each sample)
    - The Generator is the one making the fake data (it is working to fool the discriminator, to make it unable to detect the difference between the real and fake data)

- The idea behind a <b>Adversarial Autoencoder (AAE)</b>:
    - There is a part of the network that is encoding and decoding.
        - The Encoder takes input and makes a representation of it.
        - The Decoder takes that representation and recreates the original input to the best of its ability.
            - It is trying to minimize the difference between the original and new data
        - The Adversary is taking the Encoder's representation and comparing it to some distribution (normal or Gaussian or the like). It is like the Discriminator from the GAN.
            - The Encoder, then, is trying to fool the Adversary, thus changing its representations to match the chosen distribution
            - At the same time, it has to make sure that the Decoder can still recreate the original data from this representation.
        - Once trained, we can pull random values from the distribution that the Adversary has been using against the Encoder's representations to get a random representation.
            - We then feed that representation to the Decoder and allow it to create a new sample that didn't exist before

- The idea behind a <b>Variational Autoencoder (VAE)</b>:
    - This is much like the Encoder and Decoder above. 
    - The difference is that the Encoder doesn't output a whole representation. Rather, it outputs a mean and standard deviation.
    - From there, we pull a random value from a normal distribution (mean = 0, sd = 1), scale it by the outputs of the Encoder (z = sd + mean * x), and pass that to the Decoder.
    - This forces the Encoder to output more of a probability rather than a deterministic representation for each input.
    - Thus, we can make an <b>Adversarial Variational Autoencoder (AVAE)</b> like the AAE above.

- <b>Latent Space</b> is the lower-dimensional space that a model creates to represent more complex input data.
    - An Encoder takes input data and compresses it into this latent space, capturing the essentials of that data without the extra dimensions.
    - The Decoder takes this representation from the latent space and inflates it back into a higher dimension.

<!--}-->


## Thoughts about the Algorithm {#part-8}
<!--{-->

### First thoughts

- We would have to start by normalizing the data.
- What if we did something like select a certain data set to be the standard, then used an <b>Encoder</b> to take the samples in that data set and turn them all into representations in latent space.
    - We could get a distribution from those samples in the latent space
    - We could then use an <b>AAE</b>, giving the <b>Discriminator</b> the latent space distribution from our standard data set to match new data sets to
    - We set up the <b>AAE</b> to encode the samples from the new data set, trying to make the latent space representation match the distribution of the standard
    - Once we have that working, in order to get a combined data set to train on, we could either:
        - Use these latent space representations for training (which might be better since they have reduced dimensionality and may contain mostly biologically relevant data), or
        - Create a Decoder that can take these latent space representations and turn them back into our data.
            - The problem I see here is the Decoder works usually to make the new data set from the representation given by the Encoder that matches the input to the Encoder as best as it can.
            - We want to be able to combine the final data sets into one; we want them to have changed from the original data to data that contains the same information, but is similar to the standard data set.
            - So, the decoder would have to work with the latent space representations, not to turn them back into what the encoder received, but to turn them into data sets that all look alike. 
            - I don't know how we could do that.
- For testing how well we have integrated the data, I think we should do something like what Jonathan did. We can train an ensemble of classification models using the original data sets before they have been combined.
    - The labels for this would be the batch that each sample came from.
    - Then, we can run our new data set through, the one with all the data sets integrated into one.
    - If we get low accuracy on this classification, we have done a good job removing the noise from the data sets.
        - We would have to consider how we do the integration then.
            - I said that we could use the latent space representations, but then we would have to turn our unaltered batches into these representations before training on them in our classifiers
            - Or, if we convert them to expanded sets again, we wouldn't have to worry about that
    - The only other thing is that if we do this, we can see how well we have removed the noise, but how do we know if we still have good biological data?
        - I guess what Jonathan did was train an ensemble of classifiers on the new data set with their class labels and see how well it did.
        - If it performs terribly, we have not maintained signal.
    - I wonder if we could include this verification in the training process, to make our loss functions for the network based on the level of accuracy for batch classification and true biological classification (minimize the first, maximize the second)
- The problem with using an encoder to make a representation of the data in latent space based on the training data is this:
    - There are so few samples, there are so many dimensions, we won't be ab le to get a good distribution from the training data (The standard), so we can't really do that step or the further steps
- A solution to the idea of making whole data sets from the altered latent space representation:
    - We are going to have to train the encoder (and decoder) to start with to get a good distribution of the standard to compare other data sets to
    - That Decoder will know how to take the latent space representation that resembles the one for the standard (since it will be trained on the standard and try to get back the OG data set from the Encoder's representations)
    - So, we just pass in a new sample (from a data set that is not the standard), get the representation, decode it, and it should resemble the standard data set, even though it came from another data set


### Problems I see with my idea:

- If we are really going to force all data sets to have a latent space representation similar to that of the standard data set, we need a data set that has a lot of points, or the distribution will not be well defined. 
    - Does this really matter though? I am thinking that we will be processing the novel data set in batches and comparing those batches' distributions to the standard's distribution, so those batches will be small.
    - So, maybe it is okay to have a small(er) data set for the standard.
- There are a lot of dimensions in the genetic data (around 10,000). Is that going to make it impossible to even get a distribution for us to work with?
    - Maybe we could just use a Gaussian or something similar and force all data sets to look like it (rather than having a standard). 
        - I just feel like that takes away from the actual drive for the data sets to look like one another.


### More thoughts:

- The way I am thinking about this, we would only pair up two data sets at a time.
    - This could be nice because it could mean our standard would be the combined data sets and would grow larger with each add.
        - This, of course, would mean a propagation of error through all the combined data sets, but maybe if it is really good at combining, that won't be an issue.
    - The problem with this could be that it takes a lot of time for each join, which means we wouldn't want to add a ton of data sets together one by one...
        - I don't know. I guess we would have to see how long one join takes and think if we are okay with that time.


### Another thought:

- Say we have combined all our data sets that we care about. What now?
    - We want to predict on new data. We just need to create a new Encoder/Decoder, based on all of our training data.
    - We then run the new data set through the process of joining it to the training data, but we don't actually add it to the data set (all this after it has been run through the recipe to standardize it and such).
    - We get a latent space representation of our novel data set, forced to conform to the training data by our Discriminator.
    - We then run it through our Decoder to make it into a full data set again.
    - We can run this new data set through the model that we have trained on our full training data set. 
    - This is kind of like a recipe, but more extensive and intense.
- I see a problem with this, in that we may be forcing the new data to conform to our training data too much.
    - I guess we would have to see how this does in making our trained classification model generalizable to know if that is true.
- The thing is, if it is too much of a force on our novel data, we would have to find a way to make an encoder that is generalizable to all data sets, to make the latent space representation that we then use in our Decoder to get back the conforming data.


### Another problem?

- I feel like there could be a problem in generalizing the method to new data sets if we choose a standard to be our first set to which all others conform.
    - This could give us a data set in which everything is alike, but what if that makes it so all our data has the same oddities/specificities of that one batch?
    - That is where the above thought comes in. If we just force all novel data to conform to our training data, then all novel data, regardless of its platform or anything else, will follow along with our model. Does that make sense?
    - Another thing in relation to this problem:
        - What if training on the standard to get a good latent space representation and distribution causes us to fit too much to the noise in that data?
- If we do care about predicting on novel single cases, what do we do?
    - This whole algorithm seems good to combine data sets, but once we have combined the data and trained on a model, we still want to be able to predict novel input by itself, right?
        - If we have a set of data, we can make it conform (we don't need to know the class to do this, it is just with the Discriminator), and then predict on it.
        - But what if we have a single point? We can't make it conform with the discriminator, so what do we do?

### A thought about training

- It really seems like I am thinking of training many models in a row, and thus having to worry about validation and generalization at so many steps.
    - Such as in the original Encoder/Decoder
        - I worry about making the latent space distribution overfit on the standard data
            - Is that really a problem though if we just want all the data sets to be the same?
    - I worry about making the second Encoder too specific to our platforms
        - But, then again, I am thinking of having to make a new encoder for each new data set that we want to join to the whole
    - I worry about making the whole thing too specific to our data sets


### A quick thought

- For now, I am trying not to think about the fine details about loss functions and activation functions and such in training
    - I am just trying to get a good view of a high-level model that could work for what we want.


### Testing the model (the combining part of it):

- We can train on the standard before combining with the others and predict on the others.
- Then we do the combining to transform each data set.
- Then we train again on the standard (by itself this time) and test on those other sets (separately) and compare the before and after accuracy.


### A problem:

- If this method does work to combine data sets, I don't know that it will remove all the confounding variables in the data.
- The [paper about the adversarial deconfounder](#part-4) doesn't seem too focused on combining data sets, more about learning about the confounders in the data set.
- So, I don't know if our two aims are really the same. Maybe we can combine the data sets and then remove any confounders that are still there (like age or gender) that we feel still mess with the data.
- Again, this has to do with the scope of the project: are we more worried about getting the biological signal separate from other noise? Or are we more worried about combining the data sets into one that still retains biological information but comes from batches with their different noises?


### A potential biological signal problem

- If we run with my idea, will there be a problem of us using the ER status to check if biological signal is still there?
    - Will that lead to our model/algorithm only being able to keep that data of interest?
        - We will have to run the same process but use something like PR status or some other signal to see if the model still work


### More thoughts

- Other algorithms (apart from the confounders one maybe) seem to focus on taking data that came from one study.
    - They know that the data was processed in batches, but I don't know that the batch labels are necessarily in the metadata.
    - They try to make the model so that the discriminator can no longer tell the difference between batches in their embedded state.
        - They are pretty much doing PCA (though not with the PCA algorithm) to pull out the reduced data from their data set that has the biological info they want, but not the noise of the batches.
- The Confounder study does seem to take data from different studies, different data sets.
    - It, however, is focused on removing noise due to specific variables that they think are causing differences in the data (like age or gender)
    - So, it seems to be concerned about the applicability over data sets, with data set integration, more than the rest, but not in the same way.
- Our goal, as I understand it, is just to combine the data sets so they can be used together.
    - In a way, the different data sets themselves are the confounder in our approach.
    - We want to combine these data sets into one, keeping the biological signal that we want, but allowing us to predict across data sets.
    - Am I missing something? Is our approach really all that different than others'? I need to talk to Dr. Piccolo to understand if I am really understanding our idea and to see if my model is something that will do what we want.


### Model

- You can see a diagram of my idea [here](../pdfs/Model.pdf)
<!--}-->


## Further Resources {#part-9}
<!--{-->

- Other papers talking about integration with ML:
    - [16] Shaham U, Stanton KP, Zhao J, Li H, Raddassi K, Montgomery R, et al. Removal of Batch Effects Using Distribution-Matching Residual Networks. Bioinformatics (Oxford, England). 2017 Aug;33(16):2539–2546.
    - [17] Shaham U. Batch Effect Removal via Batch-Free Encoding. bioRxiv. 2018 Jul;.
- Another paper can be found [here](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010184)

<!--}-->

