---
geometry: "margin=1in"
---

# Contents
<!--{-->

1. [The Variational Fair Autoencoder](#part-1)
2. [AC-PCoA, using PCA to remove confounders](#part-2)
3. [Batch effect removal with neural nets](#part-3)
4. [Batch-free encoding](#part-4)
5. [Aligning Manifold data with GANs](#part-5)
6. [General Notes](#part-6)

<!--}-->

## The Variational Fair Autoencoder {#part-1}
<!--{-->

- A site talking about this paper can be found [here](https://ameroyer.github.io/representation%20learning/the_variational_fair_autoencoder/)
- The actual paper can be found [here](https://arxiv.org/pdf/1511.00830)

- They are trying to create a model that filters out the noise, <i>s</i>, while retaining the signal, <i>z</i>
- They are okay with using knowledge about the training labels in creating their autoencoder, as they are trying to create a model for a specific task. 
    - This is not something that I want to do. I want our model to not have to rely on the classes of the instances to be set up correctly.
    - They choose to do this because they think that the target class may have some correlation to <i>s</i>, so they don't want to remove <i>s</i> entirely
- They use the <b>Maximum Mean Discrepancy</b> in part of their training.
    - The idea behind this is to see how far off two distributions' means are
    - To get rid of <i>s</i>, they want to make the means of the positive and negative classes be the same
        - They make $p(z|s=0)$ and $p(z|s=1)$ as close to one another as possible
    - I wonder if we could incorporate this somehow in trying to conform our novel data sets to the standard.
        - I don't know quite how we would do this, though, since I have been planning on making our Discriminator do the majority of this work
- They, again, use explicit knowledge of what noise they want to remove to make their model
    - My original plan was to not need to do so
    - However, maybe I can include something in the model that allows us to do so if we want to
- From what I can tell, they are matching up their distributions by measuring their means , via MMD. Maybe we need to consider using this in our loss function in the Discriminator.
    - But also, the Discriminator doesn't need to tell the Encoder what to do in order to make it update its weights.
    - It seems to just use classification accuracy and error in its loss function
    - Yeah. I guess I am unsure what exactly they need the MMD for

<!--}-->


## AC-PCoA, Using PCA to remove confounders {#part-2}
<!--{-->

- This paper can be found [here](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010184)

- The idea here is to perform PCA to reduce the dimensionality of the data at the same time that they remove the confounders
- There was another paper that tried something similar, but their method is only effective when using Euclidean distance to talk about the relationships between points
- This paper wants to do something similar but make something that works when non-Euclidean distance is used.
    - This doesn't seem that important to me in what we are doing. 
    - Is there some way we could use PCA in our process?
        - I don't really want to because I want to keep as much of the original data as possible

<!--}-->


## Batch effect removal with neural nets {#part-3}
<!--{-->

- The paper can be found [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC5870543/)

- Here they use MMD to fit a target distribution (from one batch) to the source distribution (from another batch, the <i>standard</i>, though that's not what they call it)
- It seems like what they are doing is just trying to align the means of the two distributions
    - They can use the MMD in their loss function to get smaller loss when the distributions are more on top of one another
    - I feel like a difference between us and them is that we are trying to make the distributions match each other in more than just the mean
        - Our discriminator, I assume, will take a lot more about the distribution into consideration
- Maybe I can look into using something like the <b>Residual networks</b> they talk about in here for my Autoencoders...
- They like ResNets because they return the original value plus a little change, which is good for their problem which involves combining data that is very close to one another
    - I don't think our goal is that similar, so maybe ResNets are not the option to go with

<!--}-->


## Batch-free encoding {#part-4}
<!--{-->

- This paper can be found [here](https://www.biorxiv.org/content/10.1101/380816v1.full)

- The set up of their algorithm is as follows:
    - A shared encoder for the two batches
        - This seeks to remove all batch effects and give an embedding that has only the biological data
        - This is, in effect, learning what the biological signal is
    - A separate decoder for each batch
        - These seek to reconstruct faithfully the original data
        - They are, in effect, learning what the batch effects are
    - A discriminator
        - This seeks to distinguish between the batches
- This model is the closest thing to my idea so far
    - That being said, they use a variational autoencoder for their encoders.
    - I have only been thinking of using a standard autoencoder.
    - Maybe I want to use a variational one to better deal with novel data
        - That is something that I will need to talk to Dr. Piccolo about

<!--}-->


## Aligning Manifold Data with GANs {#part-5}
<!--{-->

- The paper can be found [here](https://proceedings.mlr.press/v80/amodio18a.html)

- The idea is to have two GANs, one for each manifold.
- Each GAN is taking points from one manifold, transforming them into an embedding, then seeing if it can distinguish from the other manifold's points
- The nice thing is that  these people have two sets of data, each sample in each manifold coming from one specific cell.
    - We do not have anything so nice in our data.
    - we don't have to worry about aligning specific points, though, so I think we are okay.
- Is there anything we could use to make sure our points are "lining up" correctly?
    - Maybe, MAYBE, we could use the idea of the platform that things were collected on?
        - But that won't work for it being generalizable.
        - And, how then do we combine things from different data sets?
        - Yeah, no.

<!--}-->


## General Notes {#part-6}
<!--{-->

### Thought about the algorithm: Generalizing to novel data sets
- We should make sure to be combining data sets from all sorts of places (for the breast cancer data sets, that means combining data sets from all the different platforms)
- After we have finished combining the very disparate data sets, we can run one last model.
    - We can put all the data sets (combined into one) into an Encoder
    - We can have the decoder match not the original data sets, but the final transformed version of them that we have now combined. 
    - Then, when we have novel data sets, we just run them through our final Autoencoder to get them in a form that looks like our other data
        - Then we can do classification and learning and such
        - We would have created a classification model based on the big combined training data set
        - This part of making the autoencoder and running through the novel data is like a recipe
    - This means that we could take into account the class we are looking for in making the whole model to get it more accurate
        - That, of course, raises the question again of if that would make our model too specific to the one question we want to answer to the point it loses all other important biological data
        - And maybe we need to do this regardless.
            - We need to make sure to keep the biological signal. But I am afraid that we will just end up overfitting to the one thing we are making sure to keep
            - But, we don't want to just make all the new data sets replicates of the standard
    - This also means that we should probably include the data sets' platform in our breast cancer data
        - We want to include all relevant features that could be noise to remove
        - We just don't want to specify which noise to remove

### Cynical Thoughts

- It seems like our algorithm, as I am envisioning it, is trying to do some sort of scaling (like normalization or standardization)
    - Maybe if I normalized the data (put it into a specific range, say from 0 to 1), there wouldn't be such a difference from platform to platform
    - After all, with the standardization that I have been doing, it seems like the RNA sequencing platforms just have more signal and thus can't be predicted on as well
- If this is true, then I don't quite understand the process, yet again...
- I need to give more thought to what exactly I am trying to do

### On how to set up the algorithm

- How do we make the discriminator actually decide if some points are from one distribution or another?
    - It is pretty much just a classifier.
    - We feed it in several points and see if it predicts the right class from them
    - We have to, therefore, give the same number of points for both the standard and novel distributions
        - How will that be affected by the size of the data sets we are integrating? I don't know.
        - How we train the Discriminator exactly, I don't know. 

<!--}-->

