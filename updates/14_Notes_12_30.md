---
geometry: "margin=1in"
---

# Contents

1. [Combining Embeddings](#part-1)
2. [Perplexity Conversation](#part-2)
3. [Progress Update](#part-3)

## Combining Embeddings {#part-1}
<!--{-->

- The paper I am reading can be found [here](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00257/43509/Learning-Multilingual-Word-Embeddings-in-Latent)

- They are trying to combine embeddings of languages, which is somewhat different than what I am envisioning for this network.
    - They want their latent space to have regions where similar words for different languages are located
        - Their latent space is pretty much a proxy for the semantics of the languages
    - I want to take a distribution in latent space and force other distributions to look the same
        - My latent space is not really supposed to hold the biological data, but to serve as a way for me to align the data sets
- I feel that this method, while interesting, may not be of use to this network.
    - If things don't work out, maybe I will come back to this.

<!--}-->

## Perplexity Conversation {#part-2}
<!--{-->

- The AI conversation I reference can be found [here](https://www.perplexity.ai/search/article-on-aligning-embedding-Ay1pI4.qShKbVCVXaw.S2A)

- All of these methods seem to be attempting to combine data sets across species, not so much gene data sets of one species
- I don't know how useful these will be as they are, but they may offer insights if my plans don't work.

<!--}-->

## Progress Update {#part-3}
<!--{-->

- I have tested various ways of normalizing the data in order to make the prediction from one platform to another better:
    - Range normalization (based on a training set)
    - Standard normalization (based on a training set)
    - No normalization
    - Range normalization (on each data set individually)
    - Standard normalization (on each data set individually)
- In addition, I have tried other ways of training the data in order to improve the prediction across platforms
    - Including many data sets from different platforms in the training set
    - Including the actual platform used as a feature in the training set
- I have tested the prediction capabilities across platforms with a reduced subset of genes
- Finally, I have compared the distribution of different data sets collected on different platforms

- Here are some conclusions:
    1. It does not matter what type of normalization I do, if based only on the training set.
        - The  metrics are the exact same for each data set regardless if I use no, range, or standard normalization
            - This may be due to the fact that I am using decision trees as my ML model.
            - If the trees have to choose to split on only some features (25 or 35, how I have it set up), the normalization may not affect the actual effect of each feature.
            - This normalization may work better to improve metrics for something like a MLP, which takes into account the size of each feature inherently.
    2. Prediction across platforms (regardless of normalization) is not great.
        - Similarly distributed data sets (like those from Affymetrix) are predicted well.
        - Data sets from different platforms (like RNA seq) are not predicted well.
        - This is to be expected, as the distributions are different.
    3. Including platform helps improve accuracy.
        - This is not an amazing improvement, especially since it required me to make a training set that had a little of each of the individual platform data sets in it.
            - This made it so my test sets were pretty much the same as my training set. 
        - Including the actual platform as a feature (rather than just having data from each platform in the training set) did nothing to improve the metrics.
            - This may again be due to the type of ML model that I was using. The tree may just not care about that feature as much as the actual gene data.
        - This is promising, as it means that we can do better than just knowing the platform that data was collected on.
        - Furthermore, I don't even like the idea of including the platform, as I don't want to have to know that information to be able to combine data sets.
            - I don't want to have to know the thing that differs between data sets before combining them. I want the model to be unsupervised, in that sense.
    4. Doing range or standard normalization improves the metrics of the model across platforms. 
        - I noticed that the data differed greatly in the actual level of expression read by the platforms.
        - I feared that my model would just aim to "force" the data into the same "range" in order to make it predict well.
        - This actual forcing of the data into the same range (or standard curve) did increase the metrics. That confirms my fear in a way.
        - My hope, though, is that my model will be able to do something more, to improve the metrics even more than just by putting them in the same range.
        - The problem here, also, is that we may want a model that can take a single point and predict it.
            - In that case, that test point can't be "normalized" all by itself. So, my model should do better than that.
    5. The CPF prediction is pretty good, even with a reduced subset of genes.
        - In fact, the prediction was better for many of the data sets.
            - This may be due to the fact that I used all of the training sets in order to find the subset of genes to use.
            - This is not really a problem, as I just wanted to know if I can use a reduced subset to make computation quicker as I test and refine my model, which I can.
    6. There are some key data sets that I will look at to really measure the performance of my model.
        - These data sets did not get better metrics in many cases, despite the fact that others did.
        - These include those like <i>GSE96058N</i> and <i>GSE62944</i>
    7. Data sets from different platforms (Affymetrix vs. RNA-seq) have different distributions, but those are somewhat overcome by different standardizations
        - This is promising, as it means that my model can overcome those differences even better and hopefully lead to even better metrics.
        - The difference will be that my model is not limited in its ability (to simple normalization or range techniques) and it will work for single points.

- To see some of these results, and what I am talking about, see the pages below:
    - [ROC Curves](../others/01_CPF_ROC_Curves.md)
    - [Metrics](../.others/02_CPF_Metrics.md)

<!--}-->

