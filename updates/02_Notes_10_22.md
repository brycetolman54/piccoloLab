---
geometry: "margin=1in"
---

# Research Notes

1. [The platforms](#part-1)
2. [ComBat](#part-2)
3. [Meta-analysis for Microarray Experiments](#part-3)
4. [Using the inSilicoMerging package](#part-4)
5. [More on XPN](#part-5)
6. [A ComBat Paper](#part-6)
7. [Cross-platform normalization works](#part-7)
8. [IMAAE: An ML model for batch effect correction](#part-8)

## The platforms {#part-1}
<!--{-->
- Affymetrix:
    - This platform grabs/detects only genes that we have specified with probes in the chips of the device. We only get the genes we want.
        - We extract RNA and give that to the array to bind and detect
    - There are several different types of this array.
    - The difference is which subset of genes are focused on, though there is a lot of overlap of genes between the arrays.
- Illumina:
    - We extract RNA, fragment it, convert it to cDNA, then sequence it and then fit together the whole sequence to find the genes present
    - Since we are measuring the actual RNA we have and reading that, we can get more genes (not just ones we specify) and measure those that are lower in abundance.
<!--}-->

## ComBat (Combining Batches) {#part-2}
<!--{-->

- The idea here is to correct for the differences between batches of data (whether that be due to using different platforms or from different runs on the same platform), to eliminate noise but keep biological signal.
- This is a <b>Bayes</b> approach, using priors to move our estimates of the variance of a batch to a smaller range of possibilities
- The idea is to take parameters $\gamma_{gj}$ and $\delta_{gj}$ for each batch (to adjust for the mean and variance, respectively, of each batch) and adjust them towards the overall mean and variance of all batches combined.
    - This is for each gene $g$ in each batch $j$
- After finding those parameters, we adjust the expression in all genes across all batches with: $Y_{gij}^{adj} = \frac{Y_{gij}-\gamma_{gj}}{\delta_{gj}}$
    - This is for each gene $g$ of the sample $i$ in batch $j$
    - We are left with adjusted gene expression levels in each batch that should make the expression comparable across the different platforms/batches
- To get the expression so we can actually combine it into one set, we have one more step:
    - We find the final expression, factoring back in biological covariates that help to keep biological signal, even with our reduction of noise.
    - The formula is: $Y_{gij}^{final} = \mu_g + X_i\beta_g + Y_{gij}^{adj}$
        - The $\mu_g$ is the average expression for that gene over all the batches
    - To get the contribution of the biological covariates $(X_i\beta_g)$:
        - $X_i$ is the design matrix:
            - It tells us about the biological factors (treatment group, age, gender, tumor stage, etc.) that could influence gene expression levels
        - $\beta_g$ is the vector of coefficients telling how much each biological factor contributes to the gene expression level of $g$
            - We can calculate this with: $Y_{ig} = X_i\beta_{g} + \epsilon_{ig}$ and a regression analysis
            - Here, $\epsilon$ is the error
- I feel like this sort of thing should work with our data, but maybe not. I need to read up on it more and get a better idea of how it is working
<!--}-->

## Meta-analysis for Microarray Experiments {#part-3}
<!--{-->

- Here is the link for the documentation of the method and an example to follow: [**Documentation**](https://www.bioconductor.org/packages/release/bioc/vignettes/GeneMeta/inst/doc/GeneMeta.pdf)

- The idea is to combine data from the same platform, maybe it would work with crossing platforms.
- It is a statistical method.
- Here are the basic equations that matter:
    - $y_i = \theta_i + \epsilon_i$
        - Where $\epsilon_i ~ N(0, \sigma_i^2)$
        - And $\theta_i = \mu + \delta_i$
            - Where $\delta_i ~ N(0, \tau^2)$
        - $\tau^2$ is the variability between studies
        - $\sigma_i^2$ is the variability within a study
        - $\mu$ is the parameter of interest (like the actual difference in mean between two sets of data)
- I am unsure what to do with this information, though. What do we do once we have the measure of the batch effects?

<!--}-->

## Using the inSilicoMerging package {#part-4}
<!--{-->

- The link for the documentation is here: [**Documentation**](https://www.bioconductor.org/packages//2.10/bioc/vignettes/inSilicoMerging/inst/doc/inSilicoMerging.pdf)
- An idea: variation due to differences in platform or technique should be seen equally across all genes and all samples in a data set. We should be able to see this variation, remove that variation from data set to data set but still have the biological data that we want. We need to somehow look at all samples and all genes at once for a data set, then compare that to the other data sets, and "mean adjust" whole data sets at a time to get rid of the differences. The question I have, how do we measure the "mean" of the data sets across genes and samples to compare to other data sets and "mean correct" the whole data set?

- There are several different methods that this function can use:
    - **Batch mean centering (BMC)**: this just subtracts the mean of a dataset from the expression of that gene in that dataset
        - This doesn't seem very robust and wouldn't keep the biological signal very well, I don't believe
    - **Combat**: This is more about this method here: [**Article**](https://academic.oup.com/biostatistics/article/8/1/118/252073)
    - **Distance-weighted discrimination (DWD)**: This is similar to *SVM*
        - This seems more complicated than *BMC*, but the same idea
        - I don't know how robust it would be, or how well it would keep biological signal
        - A little from the paper that comes [**later**](#part-6):
            - The plane that you are finding is the one that is separating the two batches that you are trying to combine. You assume they are different enough that you can find such a plane
    - **Genenorm**: This is simple z-score normalization
    - **None**: They just paste the data together
    - **Cross-platform normalization (XPN)**: the idea here is to find homogeneous clusters of genes and samples in the combined data.
        - *XPN* uses iteration to gradually update some parameters until a local minimum is found
        - The search is for terms that describe the platform-specific sensitivity, $b_i^k$, and offset, $c_i^k$
                - I then assume you scale your expression with those to get an expression you can combine with other platform observations
        - There is more about this method here: [**Article**](https://academic.oup.com/bioinformatics/article/24/9/1154/206630?login=true)

- This package combines data sets two at a time, using the intermediate sets from combining until we have only one data set
    - A question to answer here is if the end result is too much different depending on the order of combinations of the data sets

<!--}-->

## More on XPN {#part-5}
<!--{-->

- This info is coming from the paper at the following link: [**Article**](https://academic.oup.com/bioinformatics/article/24/9/1154/206630?login=true)

- One of the main ideas is that each gene expression is just a scaled/shifted version of the mean of that gene's block (the gene block is an area defined by a collection of genes/samples) => see the figure in the paper for a visual
    - We are trying to match up those gene blocks across data sets
- As such, we have to find two parameters, the scale (related to the sensitivity of each platform) and the shift (the offset is what they call it above)
- This uses *K-means* clustering to cluster the columns and rows separately, this is to define our blocks
- I feel like this is a method that would work well. If we believe so, I will go look at it further.

<!--}-->

## A ComBat paper {#part-6}
<!--{-->

- The paper I am reading from is found here: [**Article**](https://academic.oup.com/biostatistics/article/8/1/118/252073)

- Maybe for combining across platforms, we need only `step_range()` our data and then use something like ComBat.
- They define their approach as building on (introducing empirical Baye's to) a location/scale adjustment approach to combining data sets
- I feel like the thing that this method tries to do to set itself apart is focus on how they can do this all with small batch sizes (<10). That is not our problem. Maybe we could look at this approach and compare it to any other we do to see if it is actually useful.

<!--}-->

## Cross-platform normalization by non-ML means works {#part-7}
<!--{-->

- The paper I am reading is found here: [**Article**](https://www.nature.com/articles/s42003-023-04588-6)

- The general idea is to tell us that using these platforms (quantile and Training distribution matching, non-paranormal normalization and z-scores)
- However, I don't feel like that is what we are looking for exactly. It is good to know that this can be done with simpler methods

- Another paper that talks about how non-ML methods of integration work to train an ML model on various datasets is found here: [**Article**](https://www.nature.com/articles/s41526-024-00379-3)

<!--}-->

## IMAAE: An ML model to correct for batch effect {#part-8}
<!--{-->

- The paper I am reading is found here: [**Article**](https://pmc.ncbi.nlm.nih.gov/articles/PMC10056671/)

- This model looks specifically at the cell type that the genes were taken from and compares these across batches
- One batch is then chosen and all other batches are harmonized to that batch
    - Will we be able to implement something similar? Do we really have something like cell type that we can associate across the different platforms?

<!--}-->
